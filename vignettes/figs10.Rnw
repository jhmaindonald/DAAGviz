\documentclass[12pt, a4paper,  BCOR=8.25mm, DIV=15]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{newfloat}
\DeclareFloatingEnvironment[name={Supplementary Figure}]{suppfigure}

\newenvironment{itemizz}%
  {\begin{itemize}%
    \setlength{\itemsep}{2pt}%
    \setlength{\parskip}{2pt}}%
  {\end{itemize}}

\newcommand{\txtt}[1]{{\texttt{#1}}}

\begin{document}
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Map Overlays  and Spatial Modeling (Set 10)}

<<setup, cache=FALSE, echo=FALSE, purl=FALSE>>=
library(knitr)
options(replace.assign=FALSE, width=72)
opts_chunk$set(fig.path='figs/map-', cache.path='cache/map-',
               fig.align='center', dev='pdf', fig.width=3.5,
               fig.height=3.5, fig.show='hold', par=TRUE,
               tidy=FALSE,  comment=NA)
knit_hooks$set(par=function(before, options, envir){
if (before && options$fig.show!='none') par(mar=c(4,4,1.6,.1),
              cex.lab=.95,cex.axis=.9,mgp=c(2,.7,0),tcl=-.3)
}, crop=knitr::hook_pdfcrop)
pdf.options(pointsize=12)
oldopt <- options(digits=4)
@ %

\title{10: Brief Notes on Text Mining}
\author{John H Maindonald}
\maketitle

\vspace*{-0.5cm}
<<doFigs>>=
doFigs <- TRUE
@ %
\vspace*{-0.5cm}

Start by attaching required packages:

<<figset_10>>=
figset10 <- function(){
  if(!require('tm', quietly = TRUE))stop('tm must be installed')
  if(!require('latticeExtra', quietly = TRUE))stop('latticeExtra must be installed')
  if(!requireNamespace('textclean', quietly = TRUE))stop('textclean must be installed')
  if(!requireNamespace('wordcloud', quietly=TRUE))stop('wordcloud must be installed')
  }
@

Identify path to DAAGviz script directory, and read in tm-code.R .

<<getlibs, warning=FALSE, message=FALSE>>=
figset10()
@

<<accessScripts, cache=FALSE>>=
scriptdir <- system.file("scripts", package="DAAGviz")
read_chunk(paste(scriptdir,"tm-code.R",sep="/"))
@

Create paths to the text files that are stored in the
subdirectory \textbf{texts} of the \texttt{DAAGviz}.
<<paths2txt>>=
<<tobeMined>>
<<make-paths>>
@

The following reads in the first three files, creating three
text vector sources.
<<textVector, eval=FALSE, echo=TRUE>>=
## Read in the first three files, replace non-ASCII characters,
## collapse each file into one long text string, and replace
## non-ASCII characters with a space.
<<readLines1>>
<<readLines2-3>>
@

In the following, the terser directory source approach will beÂ used.
A first step, before creation of the term document matrix, is 
minimal `cleaning' of the text, enough for present purposes.

<<fig10_1>>=
fig10.1 <- function(){
<<DirSource>>
<<dirTOcorpus>>
<<make-tdm>>
<<english-stopwords>>
<<findFreq10>>
<<wordcloud1-3>>
}
@ 

Note the different scaling ranges used in
the three cases, with the large scaling range for Panel C
(\code{scale=c(9,.8)}) used to accommodate a frequency distribution
in which one item (`data') is a marked outlier.

\begin{figure}
<<tm-10_1, mfrow=c(1,3), w=15, h=5.25, echo=FALSE, out.width="0.92\\textwidth", top=5, eval=doFigs>>=
opar <- par(mar=c(4,4,1.6,3.1))
fig10.1()
par(opar)
@ %

\caption{Wordcloud plots are A: for the words in Chapter 1; B: 2; and C: 4 - 6.\label{fig:wc}}
\end{figure}

\subsection{Use of PDFs as the starting point}

Get paths to files in \textbf{pdf} subdirectory of \texttt{DAAGviz}.
<<get-path>>=
@

The function \texttt{readPDF()} creates a function that can then
be used for input of the text.  By default, it uses the `engine'
from the Poppler (\url{http://poppler.freedesktop.org/} rendering
library, which must be installed.

The following creates a corpus.

<<ex-pdf-Corpus>>=
@

This can then, in the usual way, be turned into a term document
matrix.
\subsection*{Document Collections Supplied With \texttt{tm}}
The following finds the path to the subdirectories where these 
document collections are stored:
\begin{fullwidth}
<<wid-80, echo=FALSE>>=
options(width=80)
@ %
<<pathto, eval=TRUE, echo=TRUE>>=
@ %
<<wid-54, echo=FALSE>>=
options(width=54)
@ %
\end{fullwidth}
The subdirectory \textbf{acq} has 50 Reuters documents in XML format,
\textbf{crude} has the first 23 of these, and \textbf{txt} has a
small collection of 5 text documents from the Roman poet Ovid.
These can be accessed and used for experimentation with the abilities
provided in \pkg{tm}, as required.

The following are the names of the five Ovid documents:
\begin{fullwidth}
<<wid-80, echo=FALSE>>=
@ %
<<docnames-Ovid, eval=TRUE, echo=TRUE>>=
@ %
<<wid-54, echo=FALSE>>=
@ %
\end{fullwidth}

The following brings these documents into a volatile corpus, i.e.,
a corpus that is stored in memory:
<<ovid>>=
@ %
\end{document}


